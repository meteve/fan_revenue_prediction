{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td style=\"background-color:transparent;\"><a href=\"http://www.datascience-paris-saclay.fr\">\n",
    "<img border=\"0\" src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" width=\"90%\"> </td>\n",
    "     <td style=\"background-color:transparent;\"><a href=\"https://www.sidetrade.com/\">\n",
    "<img border=\"0\" src=\"https://www.sidetrade.com/wp-content/uploads/22050384_10213210110060640_1095182809_o-300x117.png\" width=\"60%\"> </td>\n",
    "  </tr>\n",
    "</table> \n",
    "</div>\n",
    "\n",
    "<center><h1>FAN revenue prediction challenge</h1></center>\n",
    "<br/>\n",
    "<center>Lucy Liu (CDS), Maria Teleczuk (CDS), Cl√©ment Chastagnol (Sidetrade),<br /> Gael Varoquaux (Inria, Parietal), Alex Gramfort (Inria, Parietal), Guillaume Lemaitre (Scikit-learn @ Inria Foundation)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting revenue using French Attribution Notices: [RAMP studio challenge](https://ramp.studio/problems/fan_revenue_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Introduction](#Introduction)\n",
    "1. [Data](#Data)\n",
    "3. [Score metric](#Score-metric)\n",
    "4. [Data exploration](#Data-exploration)\n",
    "5. [Predictions](#Predictions)\n",
    "6. [Record linkage](#Record-linkage)\n",
    "7. [Submission structure](#Submission-structure)\n",
    "8. [Local testing](#Local-testing-(before-submission))\n",
    "9. [Submitting to RAMP studio](#Submitting-to-[ramp.studio](http://ramp.studio))\n",
    "10. [More information](#More-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this challenge is to work with 'dirty data'. Most real data is dirty and the availability of high-quality, open-source ML and data analysis frameworks (such as [scikit-learn](https://scikit-learn.org/),\n",
    "[pandas](https://pandas.pydata.org/)...) means that the next frontier for tooling and automation lies in preprocessing. This challenge aims to investigate methodologies to perform statistical analysis directly on the original dirty data.\n",
    "\n",
    "There are two datasets in this challenge:\n",
    "\n",
    "* `company_revenue_TRAIN.csv` - company revenue declarations.\n",
    "* `award_notices_RAMP.csv` -  French Attribution Notices.\n",
    "\n",
    "# Aim\n",
    "\n",
    "The predictive aim of this challenge is to use `company_revenue_TRAIN.csv` and `award_notices_RAMP.csv` to predict the Revenue for each entry in the 'company financial data' dataset. It is advised that you use both datasets, as it improves the prediction (see [Score comparison](#Score-comparison)), but using only the `company_revenue_TRAIN.csv` dataset is also allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Company financial data\n",
    "\n",
    "This dataset was built from an extract of the [National Institute of Statistics and Economic Studies (INSEE)](https://www.insee.fr/en/accueil) reference database of company revenue declarations from 2013 to 2018. Each row represents the declaration of one company for one year and the following information is provided in the columns:\n",
    "\n",
    "* `Legal_ID` - the reconcilled legal ID of the company\n",
    "* `Name` - the name of the company\n",
    "* `Activity_code (APE)` - 'Activite Principale de l'Entreprise', the main activity of the company - more information in [English](https://www.startbusinessinfrance.com/code-ape) or in [French](https://www.service-public.fr/professionnels-entreprises/vosdroits/F33050)\n",
    "* `Address` \n",
    "* `Zipcode`\n",
    "* `City`\n",
    "* `Revenue` - in Euros\n",
    "* `Headcount`\n",
    "* `Fiscal_year_end_date`\n",
    "* `Fiscal_year_duration_in_months`\n",
    "* `Year`\n",
    "\n",
    "There are a few things to note:\n",
    "\n",
    "* there are revenue declarations for the same company but different years\n",
    "* there is a large reduction in entries for the years 2017 and 2018 due to the Loi Macron law in 2017\n",
    "* the 'same company' can have several different entities, resulting in entries where the `Legal_ID` is different but the `Name`, `Address`, `City` and `Zipcode` are all the same.\n",
    "* `Revenue` can be negative. A negative revenue may be due to cancelled orders from the previous fiscal year that was recorded in the current fiscal year.\n",
    "\n",
    "\n",
    "## Award notices\n",
    "\n",
    "Every French public organisation has to issue a call for tenders when buying supplies or services (above a minimum threshold). These are called public procurement contracts. Companies then compete anonymously on these contracts and when a bid is awarded, a notice has to be legally published by the public organization on the [BOAMP](https://www.boamp.fr/) (historical data is hosted by the [DILA](https://www.dila.premier-ministre.gouv.fr/)). These are called French Attribution Notices (FAN). About 25% of awards are actually electronically published.\n",
    "\n",
    "Each contract can be divided into a maximum of 5 lots and the same company can win >1 lot of a contract. The award notices dataset comprises award notices from 2017 and 2018. Each row refers to one lot and there can be up to 5 lots referring to the same contract. The following information is provided for each lot:\n",
    "\n",
    "* `ID_call` - ID of the award notice\n",
    "* `Publication_date` of the award notice\n",
    "* `End_of_call_date` of the award notice\n",
    "* `Departments_of_publication` - the department code(s) of the award notice\n",
    "* `Department_of_provision` - the department code(s) where the contract works/goods/services were provided \n",
    "* `Call_summary` - summary of the award notice\n",
    "* `Call_title` - title of the award notice\n",
    "* `Complete_call_description` - description of the award notice\n",
    "* `Total_amount` - total amount of the contract, from all lots, in euros\n",
    "* `CPV_classes` - Common Procurement Vocabulary (CPV), a classification system for public procurement used to describe the subject of procurement contracts (more information can be found [here](https://simap.ted.europa.eu/cpv))\n",
    "Columns providing details about the company issuing the contract:\n",
    "* `Buyer_name` \n",
    "* `Buyer_address`\n",
    "* `Buyer_zipcode`\n",
    "* `Buyer_city`\n",
    "* `Buyer_email`\n",
    "* `Buyer_URL`\n",
    "\n",
    "* `Contract_awarded` - whether or not the contact was awarded\n",
    "\n",
    "Columns providing details about the winner of each lot:\n",
    "\n",
    "* `ID` - unique lot ID \n",
    "* `awarded` - whether or not the contact was awarded\n",
    "* `description` - description of the lot\n",
    "* `incumbent_name` - name of the lot winnter\n",
    "* `incumbent_address` - address of the lot winner\n",
    "* `incumbent_zipcode` - zipcode of the lot winner\n",
    "* `incumbent_city` - city of the lot winner\n",
    "* `incumbent_country` - country of the lot winner\n",
    "* `number_of_received_bids` - number of bids this lot received\n",
    "* `amount` - amount of the lot in euros\n",
    "\n",
    "**Important notes**\n",
    "\n",
    "Both data sets are very dirty. There is a lot of missing data and the column descriptions provided above are a guide only. Further, the award notices dataset is much smaller than the company revenue declarations dataset. Therefore, it is expected that many companies in the company revenue declarations dataset are not present in the award notices dataset.\n",
    "\n",
    "## Training and test\n",
    "\n",
    "The company revenue training dataset has been split into 'training' and 'test' subsets. The shapes are:\n",
    "\n",
    "* training: (1 495 948, 11)\n",
    "* test: (520 966, 11)\n",
    "\n",
    "Your model will be tested on a completely separate company revenue dataset stored on the RAMP server. This dataset has a shape of (702 181, 11). This test dataset will also be dirty but we can guarantee that the following columns will (only) be of numerical data type:\n",
    "\n",
    "* `Legal_ID`\n",
    "* `Headcount`\n",
    "* `Fiscal_year_duration_in_months`\n",
    "* `Year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score metric\n",
    "\n",
    "A unique score is used:\n",
    "\n",
    "$$score =  |max(5,log_{10}(max(1,y\\_true))) - max(5,log_{10}(max(1,y\\_pred)))| $$\n",
    "\n",
    "Score interpretation:\n",
    "\n",
    "* A lower score is better\n",
    "* Any `y_true` or `y_pred` value less than 1 is 'taken' as 1\n",
    "* If both the `y_true` and `y_pred` are less than 100 000, the score would be 0.\n",
    "* The score is the same regardless of the order of `y_true` and `y_pred` in the equation.\n",
    "* If the difference in raw `y_true` and `y_pred` values is the same, the score is greater for smaller magnitudes of `y_true` and `y_pred`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import imp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rampwf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-acadc617ea50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mproblem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Python\\Datacamp\\fan_revenue_prediction\\problem.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrampwf\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrampwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkflows\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeatureExtractorRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrampwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseScoreType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rampwf'"
     ]
    }
   ],
   "source": [
    "from problem import get_train_data\n",
    "\n",
    "X_df, y_array = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df['Fiscal_year_end_date'] = pd.to_datetime(X_df['Fiscal_year_end_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of NaN values\n",
    "X_df.isna().sum() / X_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values\n",
    "X_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRev = np.log10(np.clip(y_array, a_min=1, a_max=None))\n",
    "seaborn.kdeplot(logRev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Award notices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is present in the `data/` directory and is added by default to **all** submissions when submitting to RAMP studio. When testing your submission locally however, it is important to copy this file into the submission directory of the submission you wish to test.\n",
    "\n",
    "For example, the starting kit submission directory (`submissions/starting_kit`) contains a copy of this dataset as it is required for RAMP to work locally. You will not need to upload this data file when you are making a submission. See for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award = pd.read_csv('data/award_notices_RAMP.csv.zip', low_memory=False,\n",
    "                    compression='zip')\n",
    "award.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of NA values\n",
    "award.isna().sum() / award.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company revenue only\n",
    "\n",
    "First, let's predict using only the `comp` dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a transformer that deals with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numerical columns\n",
    "num_cols = ['Legal_ID', 'Headcount', 'Fiscal_year_duration_in_months', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='median'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a transformer to split `Fiscal_year_end_date` into separate year, month, day columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def process_date(X):\n",
    "    date = pd.to_datetime(X['Fiscal_year_end_date'], format='%Y-%m-%d')\n",
    "    return np.c_[date.dt.year, date.dt.month, date.dt.day]\n",
    "\n",
    "date_transformer = FunctionTransformer(process_date, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process the `Activity_code (APE)` column. At the moment the Scikit learn `OrdinalEncoder()` does not have a `handle_unknown` argument. This means that it would not be able to handle any values in `Activity_code (APE)` which appear in 'train' but do not appear in 'test'. Here we will simply get around this by using the first 2 characters of `APE`, which are always numbers. The first 2 numbers give the broad category the companies activities fall under (e.g. 'AGRICULTURE'). This column is then converted to numeric data type, so missing values can be dealt with by using the median value (with `SimpleImputer()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_APE(X):\n",
    "    APE = X['Activity_code (APE)'].str[:2]\n",
    "    return pd.to_numeric(APE, errors='coerce').values[:, np.newaxis]\n",
    "\n",
    "APE_transformer = FunctionTransformer(process_APE, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Only keep the numbers in the `Zipcode` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipcodes(X):\n",
    "    zipcode_nums = pd.to_numeric(X['Zipcode'], errors='coerce')\n",
    "    return zipcode_nums.values[:, np.newaxis]\n",
    "\n",
    "zipcode_transformer = FunctionTransformer(zipcodes, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the full pipeline is assembled. \n",
    "\n",
    "* for missing values in numerical columns, the 'median' is used.\n",
    "* the date column `Fiscal_year_end_date` is transformed into separated year month and day columns.\n",
    "* `Activity_code (APE)` is dealt with as described above.\n",
    "* the columns `Name`, `Address` and `City` are all dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "zipcode_col = ['Zipcode']\n",
    "date_cols = ['Fiscal_year_end_date']\n",
    "drop_cols = ['Name', 'Address', 'City']\n",
    "APE_col = ['Activity_code (APE)']\n",
    "\n",
    "\n",
    "preprocessor_comp = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('zipcode', make_pipeline(zipcode_transformer, SimpleImputer(strategy='median')), zipcode_col),\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('date', make_pipeline(date_transformer, SimpleImputer(strategy='median')), date_cols),\n",
    "        ('APE', make_pipeline(APE_transformer, SimpleImputer(strategy='median')), APE_col),\n",
    "        ('drop cols', 'drop', drop_cols),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure it works\n",
    "preprocessor_comp.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Random Forest Regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can test our model, we need to define our unique scoring function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "\n",
    "    true = np.maximum(5., np.log10(np.maximum(1., y_true)))\n",
    "    pred = np.maximum(5., np.log10(np.maximum(1., y_pred)))\n",
    "    \n",
    "    loss = np.mean(np.abs(true - pred))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "fan_loss = make_scorer(loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our model. Note that we use `GroupShuffleSplit` using `Legal_ID` as the group so that the same company ('Legal_ID') only appears in either 'train' or 'test' but does not appear both in 'train' **and** 'test'.\n",
    "\n",
    "This reflects the same conditions of this challenge where, the private 'test' data (on RAMP) does not contain any company that also appears in the public 'train' dataset you have access to. This is because `Revenue` for the same company is often very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor_comp),\n",
    "    ('classifier', regressor)])\n",
    "\n",
    "cv = GroupShuffleSplit(n_splits=5, test_size=0.25)\n",
    "\n",
    "step = 30  # to limit the running time we divide sample size by step\n",
    "scores_Xdf = -cross_val_score(clf, X_df[::step], y_array[::step],\n",
    "                              cv=cv, scoring=fan_loss,\n",
    "                              groups=X_df['Legal_ID'][::step], n_jobs=2)\n",
    "\n",
    "print(\"mean: %e (+/- %e)\" % (scores_Xdf.mean(), scores_Xdf.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive merge\n",
    "\n",
    "Now let us predict using a naive merge of `award` and `comp` datasets.\n",
    "\n",
    "The naive merge will only use the name of the company. To aid the merging we will convert the name to all lower case and remove punctuation and white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award['Name_processed'] = award['incumbent_name'].str.lower()\n",
    "award['Name_processed'] = award['Name_processed'].str.replace('[^\\w]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each company, extract 2 features:\n",
    "\n",
    "* the number of award lots won\n",
    "* the total sum of award lots won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_features = award.groupby(['Name_processed'])['amount'].agg(['count','sum'])\n",
    "award_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will perform a naive merge of `X_df` and `award_features`. \n",
    "\n",
    "Be careful in this step to ensure that the **order** of `X_df` is not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_naive(X):\n",
    "    X['Name'] = X['Name'].str.lower()     \n",
    "    X['Name'] = X['Name'].str.replace('[^\\w]','')\n",
    "    df = pd.merge(X, award_features, left_on='Name', right_on='Name_processed', how='left')\n",
    "    return df[['count','sum']]\n",
    "merge_transformer = FunctionTransformer(merge_naive, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the same feature transformation steps as above and include the 'merge' step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_col = ['Zipcode']\n",
    "date_cols = ['Fiscal_year_end_date']\n",
    "drop_cols = ['Address', 'City']\n",
    "APE_col = ['Activity_code (APE)']\n",
    "merge_col = ['Name']\n",
    "\n",
    "preprocessor_merge = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('zipcode', make_pipeline(zipcode_transformer, SimpleImputer(strategy='median')), zipcode_col),\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('date', make_pipeline(date_transformer, SimpleImputer(strategy='median')), date_cols),\n",
    "        ('APE', make_pipeline(APE_transformer, SimpleImputer(strategy='median')), APE_col),\n",
    "        ('merge', make_pipeline(merge_transformer, SimpleImputer(strategy='median')), merge_col),\n",
    "        ('drop cols', 'drop', drop_cols),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it works\n",
    "preprocessor_merge.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is implemented in RAMP within the files in the folder `submissions/starting_kit`. \n",
    "\n",
    "The transformation steps above are implemented in the file `submissions/starting_kit/feature_extractor.py` (a copy of this file is shown below). This file needs to define a class called `FeatureExtractor` which requires a `fit()` and `transform()` function. The `fit()` function takes both `X_df` and `y_array` as parameters, meaning that you are able to engineer new features using `y_array` (e.g. target encoding). The `transform()` function only takes `X_df`. We only use the `transform()` function in our simple example below.\n",
    "\n",
    "Note that the `award` dataset is being read in from the submission folder (`submissions/starting_kit`). This means that when testing locally, each submission folder should contain a copy of the award dataset `award_notices_RAMP.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y_array):\n",
    "        path = os.path.dirname(__file__)\n",
    "        award = pd.read_csv(os.path.join(path, 'award_notices_RAMP.csv.zip'),\n",
    "                            compression='zip', low_memory=False)\n",
    "        # obtain features from award\n",
    "        award['Name_processed'] = award['incumbent_name'].str.lower()\n",
    "        award['Name_processed'] = \\\n",
    "            award['Name_processed'].str.replace(r'[^\\w]', '')\n",
    "        award_features = \\\n",
    "            award.groupby(['Name_processed'])['amount'].agg(['count', 'sum'])\n",
    "\n",
    "        def zipcodes(X):\n",
    "            zipcode_nums = pd.to_numeric(X['Zipcode'], errors='coerce')\n",
    "            return zipcode_nums.values[:, np.newaxis]\n",
    "        zipcode_transformer = FunctionTransformer(zipcodes, validate=False)\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='median'))])\n",
    "\n",
    "        def process_date(X):\n",
    "            date = pd.to_datetime(X['Fiscal_year_end_date'], format='%Y-%m-%d')\n",
    "            return np.c_[date.dt.year, date.dt.month, date.dt.day]\n",
    "        date_transformer = FunctionTransformer(process_date, validate=False)\n",
    "\n",
    "        def process_APE(X):\n",
    "            APE = X['Activity_code (APE)'].str[:2]\n",
    "            return pd.to_numeric(APE).values[:, np.newaxis]\n",
    "        APE_transformer = FunctionTransformer(process_APE, validate=False)\n",
    "\n",
    "        def merge_naive(X):\n",
    "            X['Name'] = X['Name'].str.lower()\n",
    "            X['Name'] = X['Name'].str.replace(r'[^\\w]', '')\n",
    "            df = pd.merge(X, award_features, left_on='Name',\n",
    "                          right_on='Name_processed', how='left')\n",
    "            return df[['count', 'sum']]\n",
    "        merge_transformer = FunctionTransformer(merge_naive, validate=False)\n",
    "\n",
    "        num_cols = ['Legal_ID', 'Headcount',\n",
    "                    'Fiscal_year_duration_in_months', 'Year']\n",
    "        zipcode_col = ['Zipcode']\n",
    "        date_cols = ['Fiscal_year_end_date']\n",
    "        APE_col = ['Activity_code (APE)']\n",
    "        merge_col = ['Name']\n",
    "        drop_cols = ['Address', 'City']\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('zipcode', make_pipeline(zipcode_transformer,\n",
    "                 SimpleImputer(strategy='median')), zipcode_col),\n",
    "                ('num', numeric_transformer, num_cols),\n",
    "                ('date', make_pipeline(date_transformer,\n",
    "                 SimpleImputer(strategy='median')), date_cols),\n",
    "                ('APE', make_pipeline(APE_transformer,\n",
    "                 SimpleImputer(strategy='median')), APE_col),\n",
    "                ('merge', make_pipeline(merge_transformer,\n",
    "                 SimpleImputer(strategy='median')), merge_col),\n",
    "                ('drop cols', 'drop', drop_cols),\n",
    "                ])\n",
    "\n",
    "        self.preprocessor = preprocessor\n",
    "        self.preprocessor.fit(X_df, y_array)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        return self.preprocessor.transform(X_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Random Forest Regressor model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implemented in the file `submissions/starting_kit/regressor.py` (a copy of this file is shown below). The `Regressor` class must have a `fit()` and `predict()` function. If you are using a scikit-learn function, this can be done by simply calling `fit()` and `predict()` on the regressor, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.reg = RandomForestRegressor(n_estimators=5)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our model within this notebook, using `preprocessor_merge` and `regressor` defined above, in a `Pipeline()`. \n",
    "\n",
    "Once you are happy with a solution, you can transfer your solution to `feature_extractor.py` and `regressor.py` files and test your submission using RAMP (see 'Submissions')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor_merge),\n",
    "    ('classifier', regressor)])\n",
    "\n",
    "cv = GroupShuffleSplit(n_splits=5, test_size=0.25)\n",
    "\n",
    "step = 30  # to limit the running time we divide sample size by step\n",
    "scores_merge = -cross_val_score(clf, X_df[::step], y_array[::step], cv=cv,\n",
    "                                scoring=fan_loss,\n",
    "                                groups=X_df['Legal_ID'][::step],\n",
    "                                n_jobs=2)\n",
    "\n",
    "print(\"mean: %e (+/- %e)\" % (scores_merge.mean(), scores_merge.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score comparison\n",
    "\n",
    "You can see in the below plot of the scores that using the merged data always results in better scores (smaller is better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({'Xdf': scores_Xdf, 'merge': scores_merge})\n",
    "scores.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record linkage\n",
    "\n",
    "The problem of trying to match 'records' referring to the same entity across different data sources is called  record linkage. We used a very naive way to match companies above but there are much more sophiisticated methods and packages implementing these methods. To get you started here are two Python packages:\n",
    "\n",
    "* [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/index.html)\n",
    "* [Dedupe](https://github.com/dedupeio/dedupe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission structure\n",
    "\n",
    "Each of your submissions should be in it's own folder within the `submissions` folder (e.g. `submissions/my_submission`). The submission directory should contain 3 files:\n",
    "\n",
    "* a copy of the `award_notices_RAMP.csv` dataset. Note that you will **not** need to upload this file when making a submission in RAMP studio as this is done automatically by RAMP studio.\n",
    "* `feature_extractor.py` - this should merge the `X_df` and the award dataset and any feature transformation you wish\n",
    "* `regressor.py` - this should implement a regressor with a `fit()` and `predict()` function\n",
    "\n",
    "See `submissions/starting_kit` for an example.\n",
    "\n",
    "You can also test your submissions using RAMP-workflow before submitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. For this we provide a unit test - `ramp_test_submission`. \n",
    "\n",
    "First, ensure that `ramp-worflow` is installed (see the [github repo](https://github.com/paris-saclay-cds/ramp-workflow) for installation instructions). \n",
    "\n",
    "Now you can use `ramp_test_submission`. This command will test on files in [`submissions/starting_kit`](/submissions/starting_kit) by default. To specify testing on a different folder use the flag `--submission`. For example to run the test on `submissions/solution1` use: `ramp_test_submission --submission solution1`.\n",
    "\n",
    "If it runs and print training and test errors on each fold, then you can submit the code.\n",
    "\n",
    "For example, below we test the starting kit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `--quick-test` argument to test on the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good model, you can submit it to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then sign up for the 'FAN_revenue_prediction' challenge and the 'Saclay M2 Data Camp 2019/20' [event](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19). Both signups are controlled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19/sandbox). Here you can either edit and save the code files on the left hand side of the page or upload your `feature_extractor.py` and `regressor.py` files on the right hand side of the page, then give your submission a name and click 'submit'. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19/my_submissions). Once it is trained, you get an email, and your submission shows up on the [public leaderboard](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_ then check that your submission works locally with `ramp_test_submission`. The script will print mean cross-validation scores if your submission works.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](https://ramp.studio/events/fan_revenue_prediction_saclay_datacamp_19/leaderboard)) is `FAN error`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
